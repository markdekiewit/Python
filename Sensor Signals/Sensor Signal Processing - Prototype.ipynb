{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensos Signal Processing - Prototype\n",
    "\n",
    "The purpose of this notebook is to prototype the processing of a sensor signal where 5 consecutive values above an upper threshold or 5 consecutive values below a lower threshold result in an event being recorded.\n",
    "\n",
    "To simulate a signal this notebook will make use of an array of random numbers between 1 and 10\n",
    "An event is the fact that there have been 5 consecutive signals surpassing the upper or lower threshold. Signals surpassing the upper threshold will result in a positive event. Signals surpassing the lower threshold will result in a negative event.\n",
    "An event consists of an ID, an event type indicating it is a positive or negative event, and the array index of the last occurrance where the signal surpassed the threshold.\n",
    "\n",
    "The assumption for this prototype is that after an event has been created, thus 5 consecutive signals have been processed, and the next signal also surpasses the threshold then this would be the first occurance of the next event (providing there are 4 more consecutive signals surpassing the threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = np.random.randint(1,10,100000)\n",
    "upperThreshold = 7\n",
    "lowerThreshold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "9\n",
      "9\n",
      "7\n",
      "5\n",
      "5\n",
      "1\n",
      "4\n",
      "7\n",
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "test = np.random.randint(1,10,10)\n",
    "for i in test:\n",
    "    print(i)\n",
    "\n",
    "print(len(test))\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventLog = [] # create empty event log list to collect events\n",
    "signalCount = 0\n",
    "index = 0\n",
    "\n",
    "#check\n",
    "#print(eventLog)\n",
    "#print(len(eventLog))\n",
    "#eventLog.append([len(eventLog)+1, ('upper', 7)])\n",
    "#print(len(eventLog))\n",
    "#print(eventLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in signal:\n",
    "    index = index + 1\n",
    "    if i >= lowerThreshold and i <= upperThreshold:\n",
    "        signalCount = 0 # reset count of signals\n",
    "    elif i > upperThreshold and signalCount <= 0:\n",
    "        signalCount = 1 # first signal of upper event\n",
    "    elif i < lowerThreshold and signalCount >= 0:\n",
    "        signalCount = -1 # first signal of lower event\n",
    "    elif i > upperThreshold and signalCount < 4:\n",
    "        signalCount = signalCount + 1 # next signal is also over upper threshold, but not yet reached 5 consecutive signals\n",
    "    elif i < lowerThreshold and signalCount > -4:\n",
    "        signalCount = signalCount - 1 # next signal is also under lower threshold, but not yet reached 5 consecutive signals\n",
    "    elif i > upperThreshold and signalCount == 4:\n",
    "        eventLog.append([len(eventLog)+1, ('upper', index)]) # insert upper event in event log\n",
    "        signalCount = 0 # set signal count back to zero for next event\n",
    "    elif i < lowerThreshold and signalCount == -4:\n",
    "        eventLog.append([len(eventLog)+1, ('lower', index)]) # insert lower event in event log\n",
    "        signalCount = 0 # set signal count back to zero for next event\n",
    "    else:\n",
    "        print(i)\n",
    "        print('unexpected result - review code')\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next version to include sensor files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required lybraries/functions\n",
    "import datetime\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for this program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy sensor files\n",
    "\n",
    "for i in range(10):\n",
    "    dt = datetime.datetime(2019, 11, 3) + datetime.timedelta(minutes = 1*i)\n",
    "    filename = 'sensors/sensor1/sensor1_' + dt.strftime('%Y%m%d%H%M%S') + '.txt'\n",
    "    #print(filename)\n",
    "    file = open(filename, 'a')\n",
    "    #file.write('sensorid, datetime, value\\n')\n",
    "    file.write('sensor1,'+ dt.strftime('%Y%m%d%H%M%S') + ',' + str(randint(1,10)) + '\\n')\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sensor1_20191103000000.txt', 'sensor1_20191103000100.txt', 'sensor1_20191103000200.txt', 'sensor1_20191103000300.txt', 'sensor1_20191103000400.txt', 'sensor1_20191103000500.txt', 'sensor1_20191103000600.txt', 'sensor1_20191103000700.txt', 'sensor1_20191103000800.txt', 'sensor1_20191103000900.txt']\n"
     ]
    }
   ],
   "source": [
    "# read existing sensor files from direcory\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = 'sensors/sensor1/'\n",
    "\n",
    "sensorFiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "print(sensorFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor1,20191103000000,5\n",
      "sensor1,20191103000100,5\n",
      "sensor1,20191103000200,2\n",
      "sensor1,20191103000300,5\n",
      "sensor1,20191103000400,8\n",
      "sensor1,20191103000500,10\n",
      "sensor1,20191103000600,9\n",
      "sensor1,20191103000700,4\n",
      "sensor1,20191103000800,4\n",
      "sensor1,20191103000900,1\n"
     ]
    }
   ],
   "source": [
    "#test read files\n",
    "for f in sensorFiles:\n",
    "    mf = mypath + f\n",
    "    with open(mf,'r') as sf:\n",
    "        lines = sf.readlines()\n",
    "        #print(lines)\n",
    "        for l in lines:\n",
    "            l = l.rstrip()\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.stop of <SparkContext master=local[4] appName=Sensor Signal Processing Prototype>>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sc.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Sensor Signal Processing Prototype, master=local[4]) created by __init__ at <ipython-input-19-f12ff0fb290c>:6 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f12ff0fb290c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sensor Signal Processing Prototype\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[4]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Sensor Signal Processing Prototype, master=local[4]) created by __init__ at <ipython-input-19-f12ff0fb290c>:6 "
     ]
    }
   ],
   "source": [
    "#Create Spark Session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pandas as pd\n",
    "\n",
    "conf = SparkConf().setAppName(\"Sensor Signal Processing Prototype\").setMaster(\"local[4]\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiewi\\Documents\\python\\Notebooks\\sensors\\sensor1\\sensor*.txt MapPartitionsRDD[8] at textFile at <unknown>:0\n",
      "['sensor1,20191103000000,5', 'sensor1,20191103000100,5', 'sensor1,20191103000200,2']\n"
     ]
    }
   ],
   "source": [
    "#Read files into DataFrame\n",
    "\n",
    "sensorReadings = sc.textFile(r\"C:\\Users\\kiewi\\Documents\\python\\Notebooks\\sensors\\sensor1\\sensor*.txt\")\n",
    "print(sensorReadings)\n",
    "print(sensorReadings.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('sensor1', '20191103000000'), '5'), (('sensor1', '20191103000100'), '5'), (('sensor1', '20191103000200'), '2')]\n"
     ]
    }
   ],
   "source": [
    "#create (key, value) pair and sort on key\n",
    "\n",
    "srSorted = sensorReadings.map(lambda x: x.split(',')).map(lambda x: ((x[0],x[1]),x[2]))\n",
    "\n",
    "print(srSorted.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
